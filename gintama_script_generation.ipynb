{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing...\n",
    "import numpy as np\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the gintama script file\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 41322\n",
      "Number of lines: 56992\n",
      "Average number of words in each line: 7.141774284110051\n",
      "\n",
      "The lines 0 to 10:\n",
      "Kagura: There were also many other \\Nstaff who were affected.\n",
      "Kagura: and *****-san.\n",
      "All: We're very sorry!\n",
      "GINTOKI: Time to commit seppuku.\n",
      "GINTOKI: You're dressed in white \\Nso you're ready to die.\n",
      "shinpachi: Why only me?!\n",
      "shinpachi: We're all dressed in white!\n",
      "Kagura: What's wrong?\n",
      "Kagura: It's time for you to take responsibility\n",
      "Kagura: for turning the eighth \\Nepisode of the second series\n"
     ]
    }
   ],
   "source": [
    "# exploring data\n",
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tokens = dict()\n",
    "    tokens['.'] = '||period||'\n",
    "    tokens[','] = '||comma||'\n",
    "    tokens['\"'] = '||quotation_mark||'\n",
    "    tokens[';'] = '||semicolon||'\n",
    "    tokens['!'] = '||exclam_mark||'\n",
    "    tokens['?'] = '||question_mark||'\n",
    "    tokens['('] = '||left_par||'\n",
    "    tokens[')'] = '||right_par||'\n",
    "    tokens['-'] = '||dash||'\n",
    "    tokens['\\n'] = '||return||'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "token_dict = token_lookup()\n",
    "for key,token in token_dict.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "int_text = [vocab_to_int[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check access to gpu\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    data_x = np.zeros((len(words)-sequence_length, sequence_length), dtype='int64')\n",
    "    data_y = np.zeros((len(words)-sequence_length, 1), dtype='int64')\n",
    "    for i in range(len(words) - sequence_length):\n",
    "        data_x[i] = words[i:sequence_length+i]\n",
    "        data_y[i] = words[i+sequence_length:i+sequence_length+1]\n",
    "    data_y = np.ndarray.flatten(data_y)\n",
    "    dataset = TensorDataset(torch.from_numpy(data_x),torch.from_numpy(data_y))\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)  \n",
    "    # return a dataloader\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        # set class variables\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # define model layers\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        batch_size= nn_input.size(0)\n",
    "        nn_input = nn_input.long()\n",
    "        nn_input = self.embed(nn_input)\n",
    "        lstm_out,hidden = self.lstm(nn_input,hidden)\n",
    "        #output = self.dropout(lstm_out)\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        output = output[:, -1] # get last batch of labels\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if (train_on_gpu):\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    output, hidden = rnn(inp,hidden)\n",
    "    loss = criterion(output.squeeze(), target)\n",
    "    loss.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 4)\n",
    "    optimizer.step()\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    \n",
    "    return loss.item(), hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10 # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 256\n",
    "# Hidden Dimension\n",
    "hidden_dim = int(256*1.5) \n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15 epoch(s)...\n",
      "Epoch:    1/15    Loss: 5.145742391586304\n",
      "\n",
      "Epoch:    1/15    Loss: 4.596281274080276\n",
      "\n",
      "Epoch:    2/15    Loss: 4.288210717727797\n",
      "\n",
      "Epoch:    2/15    Loss: 4.182242734193802\n",
      "\n",
      "Epoch:    3/15    Loss: 3.9634728665879972\n",
      "\n",
      "Epoch:    3/15    Loss: 3.913464971899986\n",
      "\n",
      "Epoch:    4/15    Loss: 3.7257964204373435\n",
      "\n",
      "Epoch:    4/15    Loss: 3.710173046588898\n",
      "\n",
      "Epoch:    5/15    Loss: 3.5420918574522076\n",
      "\n",
      "Epoch:    5/15    Loss: 3.548871638417244\n",
      "\n",
      "Epoch:    6/15    Loss: 3.3951078974246593\n",
      "\n",
      "Epoch:    6/15    Loss: 3.4106796110868456\n",
      "\n",
      "Epoch:    7/15    Loss: 3.270848271050758\n",
      "\n",
      "Epoch:    7/15    Loss: 3.311476106762886\n",
      "\n",
      "Epoch:    8/15    Loss: 3.1743665154942025\n",
      "\n",
      "Epoch:    8/15    Loss: 3.224072265267372\n",
      "\n",
      "Epoch:    9/15    Loss: 3.086100681423659\n",
      "\n",
      "Epoch:    9/15    Loss: 3.1342916858196257\n",
      "\n",
      "Epoch:   10/15    Loss: 3.0234283503738255\n",
      "\n",
      "Epoch:   10/15    Loss: 3.0779945914745332\n",
      "\n",
      "Epoch:   11/15    Loss: 2.9589432012485513\n",
      "\n",
      "Epoch:   11/15    Loss: 3.0191894228458405\n",
      "\n",
      "Epoch:   12/15    Loss: 2.9127109109536233\n",
      "\n",
      "Epoch:   12/15    Loss: 2.959023904800415\n",
      "\n",
      "Epoch:   13/15    Loss: 2.8588072883572035\n",
      "\n",
      "Epoch:   13/15    Loss: 2.9213820538520814\n",
      "\n",
      "Epoch:   14/15    Loss: 2.822985357590616\n",
      "\n",
      "Epoch:   14/15    Loss: 2.8761927164793013\n",
      "\n",
      "Epoch:   15/15    Loss: 2.7790794988360825\n",
      "\n",
      "Epoch:   15/15    Loss: 2.8416951855421066\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-786ae92fbd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./save/trained_rnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Trained and Saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "torch.save(trained_rnn,'trained_rnn.pt')\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(trained_rnn,'trained_rnn.pt')\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagura: you can do!\n",
      "shinpachi: what are you going to do?\n",
      "shinpachi: what do we do?\n",
      "gintoki: don't you understand?\n",
      "kagura: you should go to sleep.\n",
      "kagura: i don't care what happens\\nto you geezers anymore.\n",
      "hijikata: i won't allow you to beat\\nme to this world.\n",
      "kagura: that's why i told you to stop following my hair suffer.\n",
      "shinpachi: the benizakura arc has\\nnothing...\n",
      "yamazaki:... a woman with a lady that\\nhelps children.\n",
      "gintoki: what?\n",
      "kagura: i\n"
     ]
    }
   ],
   "source": [
    "gen_length = 100 # modify the length to your preference\n",
    "prime_word = 'kagura' # name for starting the script\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagura: you can go!\n",
      "gintoki: don't give me that shit!\n",
      "kagura: i won't let him!\n",
      "kagura: i'm sorry. i have no idea \\nwhat i should\\ndo.\n",
      "gintoki: what kind of education are you going to keep talking to us!\n",
      "shinpachi: what?!\n",
      "shinpachi: why do we have to be stuck with\\nmr?\n",
      "kagura: it's the 31st.\n",
      "shinpachi: i don't know how to show\\nmy appreciation or that...\n",
      "gintoki: i didn't want you to die.\n",
      "gintoki: i don't remember asking you.\n"
     ]
    }
   ],
   "source": [
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagura:?!\n",
      "gintoki: don't give me your copy!\n",
      "shinpachi: that's not the point!\n",
      "shinpachi: what?!\n",
      "shinpachi: what do you think of\\nsomething like that is?\n",
      "gintoki: what kind of errands are you?\n",
      "gintoki: it's a pretty girl meeting.\n",
      "gintoki: i don't want to see you through this thing.\n",
      "shinpachi: you don't have to say it...\n",
      "shinpachi: that's enough.\n",
      "shinpachi: that's not the issue.\n",
      "shinpachi: i don't know anything about the chain shoots.\n",
      "gintoki: huh?\n"
     ]
    }
   ],
   "source": [
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
